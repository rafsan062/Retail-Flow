{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/htnphu/retail-demand-forecasting/blob/main/m5_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-ikABAQIGdJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import altair as alt\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "import gc\n",
        "\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIP6E79WIYAC",
        "outputId": "350946f4-5897-4430-b98c-534788add599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHajpZh8IZgm"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/Fall_2025/CPSC_5305_Intro_to_DS/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVZG8G4BIccv"
      },
      "outputs": [],
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        # --- FIX: Skip Datetime Columns ---\n",
        "        if pd.api.types.is_datetime64_any_dtype(col_type):\n",
        "            continue\n",
        "        # -----------------------------------\n",
        "\n",
        "        # Only process numeric columns for min/max\n",
        "        if col_type != object and not isinstance(col_type, pd.CategoricalDtype):\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                # This is where the error occurred previously for datetime types\n",
        "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        # Handle categorical columns without min/max\n",
        "        elif isinstance(col_type, pd.CategoricalDtype):\n",
        "            df[col] = df[col].cat.as_unordered()  # Ensure no ordering assumption\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')  # Convert objects to category\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose:\n",
        "        print(f'Memory usage decreased from {start_mem:.2f} Mb to {end_mem:.2f} Mb ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-_179NgId-I",
        "outputId": "5834d7b2-2d91-4826-d4c2-b2c7f9963ce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and reducing memory usage for data files...\n",
            "Memory usage decreased from 13093.96 Mb to 2767.02 Mb (78.9% reduction)\n",
            "\n",
            "Initial data loading complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading and reducing memory usage for data files...\")\n",
        "\n",
        "# load the datasets and apply memory reduction\n",
        "df = pd.read_csv(f'{DATA_PATH}m5_processed.csv')\n",
        "df = reduce_mem_usage(df)\n",
        "\n",
        "print(\"\\nInitial data loading complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxiko-TzIfJf",
        "outputId": "c9676935-1c37-4074-e52b-ccf0ee5f3b26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|    | id                          | item_id     | dept_id   | cat_id   | store_id   | state_id   | d   |   sales |   wm_yr_wk |   sell_price | date       | weekday   |   wday |   month |   year |   snap_CA |   snap_TX |   snap_WI |   day_of_week |   day_of_month |   day_of_year |   weekend | event_name_1   | event_type_1   | event_name_2   | event_type_2   |   price_mean |   price_relative_to_mean |   price_change_lag |\n",
            "|---:|:----------------------------|:------------|:----------|:---------|:-----------|:-----------|:----|--------:|-----------:|-------------:|:-----------|:----------|-------:|--------:|-------:|----------:|----------:|----------:|--------------:|---------------:|--------------:|----------:|:---------------|:---------------|:---------------|:---------------|-------------:|-------------------------:|-------------------:|\n",
            "|  0 | FOODS_1_001_CA_1_evaluation | FOODS_1_001 | FOODS_1   | FOODS    | CA_1       | CA         | d_1 |       3 |      11101 |            2 | 2011-01-29 | Saturday  |      1 |       1 |   2011 |         0 |         0 |         0 |             5 |             29 |            29 |         1 | No event       | No event       | No event       | No event       |      2.16816 |                 0.922441 |                  0 |\n",
            "|  1 | FOODS_1_001_CA_1_evaluation | FOODS_1_001 | FOODS_1   | FOODS    | CA_1       | CA         | d_2 |       0 |      11101 |            2 | 2011-01-30 | Sunday    |      2 |       1 |   2011 |         0 |         0 |         0 |             6 |             30 |            30 |         1 | No event       | No event       | No event       | No event       |      2.16816 |                 0.922441 |                  0 |\n",
            "|  2 | FOODS_1_001_CA_1_evaluation | FOODS_1_001 | FOODS_1   | FOODS    | CA_1       | CA         | d_3 |       0 |      11101 |            2 | 2011-01-31 | Monday    |      3 |       1 |   2011 |         0 |         0 |         0 |             0 |             31 |            31 |         0 | No event       | No event       | No event       | No event       |      2.16816 |                 0.922441 |                  0 |\n",
            "|  3 | FOODS_1_001_CA_1_evaluation | FOODS_1_001 | FOODS_1   | FOODS    | CA_1       | CA         | d_4 |       1 |      11101 |            2 | 2011-02-01 | Tuesday   |      4 |       2 |   2011 |         1 |         1 |         0 |             1 |              1 |            32 |         0 | No event       | No event       | No event       | No event       |      2.16816 |                 0.922441 |                  0 |\n",
            "|  4 | FOODS_1_001_CA_1_evaluation | FOODS_1_001 | FOODS_1   | FOODS    | CA_1       | CA         | d_5 |       4 |      11101 |            2 | 2011-02-02 | Wednesday |      5 |       2 |   2011 |         1 |         0 |         1 |             2 |              2 |            33 |         0 | No event       | No event       | No event       | No event       |      2.16816 |                 0.922441 |                  0 |\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(df.head().to_markdown())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTLBhZHbIjFw",
        "outputId": "544ff87c-a91a-4d52-b4cf-3a4fccff2792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 59181090 entries, 0 to 59181089\n",
            "Data columns (total 29 columns):\n",
            " #   Column                  Dtype   \n",
            "---  ------                  -----   \n",
            " 0   id                      category\n",
            " 1   item_id                 category\n",
            " 2   dept_id                 category\n",
            " 3   cat_id                  category\n",
            " 4   store_id                category\n",
            " 5   state_id                category\n",
            " 6   d                       category\n",
            " 7   sales                   int16   \n",
            " 8   wm_yr_wk                int16   \n",
            " 9   sell_price              float32 \n",
            " 10  date                    category\n",
            " 11  weekday                 category\n",
            " 12  wday                    int8    \n",
            " 13  month                   int8    \n",
            " 14  year                    int16   \n",
            " 15  snap_CA                 int8    \n",
            " 16  snap_TX                 int8    \n",
            " 17  snap_WI                 int8    \n",
            " 18  day_of_week             int8    \n",
            " 19  day_of_month            int8    \n",
            " 20  day_of_year             int16   \n",
            " 21  weekend                 int8    \n",
            " 22  event_name_1            category\n",
            " 23  event_type_1            category\n",
            " 24  event_name_2            category\n",
            " 25  event_type_2            category\n",
            " 26  price_mean              float32 \n",
            " 27  price_relative_to_mean  float32 \n",
            " 28  price_change_lag        float32 \n",
            "dtypes: category(13), float32(4), int16(4), int8(8)\n",
            "memory usage: 2.7 GB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o0OZBe9IkOB",
        "outputId": "96ef80ec-5040-45a0-9209-7b7ec9209f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(59181090, 29)\n"
          ]
        }
      ],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjVdDS5oImSu",
        "outputId": "ff799d2b-24e1-4a44-fc76-2b39fe284368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|                        |   0 |\n",
            "|:-----------------------|----:|\n",
            "| id                     |   0 |\n",
            "| item_id                |   0 |\n",
            "| dept_id                |   0 |\n",
            "| cat_id                 |   0 |\n",
            "| store_id               |   0 |\n",
            "| state_id               |   0 |\n",
            "| d                      |   0 |\n",
            "| sales                  |   0 |\n",
            "| wm_yr_wk               |   0 |\n",
            "| sell_price             |   0 |\n",
            "| date                   |   0 |\n",
            "| weekday                |   0 |\n",
            "| wday                   |   0 |\n",
            "| month                  |   0 |\n",
            "| year                   |   0 |\n",
            "| snap_CA                |   0 |\n",
            "| snap_TX                |   0 |\n",
            "| snap_WI                |   0 |\n",
            "| day_of_week            |   0 |\n",
            "| day_of_month           |   0 |\n",
            "| day_of_year            |   0 |\n",
            "| weekend                |   0 |\n",
            "| event_name_1           |   0 |\n",
            "| event_type_1           |   0 |\n",
            "| event_name_2           |   0 |\n",
            "| event_type_2           |   0 |\n",
            "| price_mean             |   0 |\n",
            "| price_relative_to_mean |   0 |\n",
            "| price_change_lag       |   0 |\n"
          ]
        }
      ],
      "source": [
        "print(df.isnull().sum().to_markdown())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(by=['id', 'date'], inplace=True)"
      ],
      "metadata": {
        "id": "_3ymQI5P7n9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head().to_markdown())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fdujbDl7xOg",
        "outputId": "bf188ac0-6cc6-4908-f8ee-6df1ef6dea84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|    | id                          | item_id     | dept_id   | cat_id   | store_id   | state_id   | d   |   sales |   wm_yr_wk |   sell_price | date       | weekday   |   wday |   month |   year |   snap_CA |   snap_TX |   snap_WI |   day_of_week |   day_of_month |   day_of_year |   weekend | event_name_1   | event_type_1   | event_name_2   | event_type_2   |   price_mean |   price_relative_to_mean |   price_change_lag |\n",
            "|---:|:----------------------------|:------------|:----------|:---------|:-----------|:-----------|:----|--------:|-----------:|-------------:|:-----------|:----------|-------:|--------:|-------:|----------:|----------:|----------:|--------------:|---------------:|--------------:|----------:|:---------------|:---------------|:---------------|:---------------|-------------:|-------------------------:|-------------------:|\n",
            "|  0 | FOODS_1_001_CA_1_evaluation | FOODS_1_001 | FOODS_1   | FOODS    | CA_1       | CA         | d_1 |       3 |      11101 |            2 | 2011-01-29 | Saturday  |      1 |       1 |   2011 |         0 |         0 |         0 |             5 |             29 |            29 |         1 | No event       | No event       | No event       | No event       |      2.16816 |                 0.922441 |                  0 |\n",
            "|  1 | FOODS_1_001_CA_1_evaluation | FOODS_1_001 | FOODS_1   | FOODS    | CA_1       | CA         | d_2 |       0 |      11101 |            2 | 2011-01-30 | Sunday    |      2 |       1 |   2011 |         0 |         0 |         0 |             6 |             30 |            30 |         1 | No event       | No event       | No event       | No event       |      2.16816 |                 0.922441 |                  0 |\n",
            "|  2 | FOODS_1_001_CA_1_evaluation | FOODS_1_001 | FOODS_1   | FOODS    | CA_1       | CA         | d_3 |       0 |      11101 |            2 | 2011-01-31 | Monday    |      3 |       1 |   2011 |         0 |         0 |         0 |             0 |             31 |            31 |         0 | No event       | No event       | No event       | No event       |      2.16816 |                 0.922441 |                  0 |\n",
            "|  3 | FOODS_1_001_CA_1_evaluation | FOODS_1_001 | FOODS_1   | FOODS    | CA_1       | CA         | d_4 |       1 |      11101 |            2 | 2011-02-01 | Tuesday   |      4 |       2 |   2011 |         1 |         1 |         0 |             1 |              1 |            32 |         0 | No event       | No event       | No event       | No event       |      2.16816 |                 0.922441 |                  0 |\n",
            "|  4 | FOODS_1_001_CA_1_evaluation | FOODS_1_001 | FOODS_1   | FOODS    | CA_1       | CA         | d_5 |       4 |      11101 |            2 | 2011-02-02 | Wednesday |      5 |       2 |   2011 |         1 |         0 |         1 |             2 |              2 |            33 |         0 | No event       | No event       | No event       | No event       |      2.16816 |                 0.922441 |                  0 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjmNBqZV_e9S",
        "outputId": "fcc15302-f183-46d7-8e7b-f71241ca1cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66CnGm1V4oh-"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdjONQzQ4pv4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import lightgbm as lgb\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# features engineering\n",
        "# create features grouped by item ('id')\n",
        "# .shift(28) looks 28 days back *for that item\n",
        "df['sales_lag_7'] = df.groupby('id')['sales'].shift(7)\n",
        "df['sales_lag_28'] = df.groupby('id')['sales'].shift(28)\n",
        "\n",
        "# .rolling(28) gets a 28-day window for that item\n",
        "# .shift(1) ensures we only use data from before the current day\n",
        "df['sales_rolling_mean_28'] = df.groupby('id')['sales'].shift(1).rolling(28).mean()\n",
        "print(\"New features created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSkEwKr-3ahc",
        "outputId": "7df6abfa-84fe-419f-e273-e67ec75bc4a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New features created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
        "                        'weekday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
        "\n",
        "numeric_features = ['wday', 'month', 'year', 'day_of_week', 'day_of_month', 'day_of_year',\n",
        "                    'weekend', 'snap_CA', 'snap_TX', 'snap_WI',\n",
        "                    'sell_price', 'price_mean', 'price_relative_to_mean', 'price_change_lag', 'sales_lag_7', 'sales_lag_28', 'sales_rolling_mean_28']\n",
        "\n",
        "features = categorical_features + numeric_features\n",
        "\n",
        "target = 'sales'\n",
        "\n",
        "print(f\"Original shape before dropna: {df.shape}\")\n",
        "df.dropna(inplace=True)\n",
        "print(f\"New shape after dropna: {df.shape}\")\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "n_splits = 5\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "print(f\"\\nStarting {n_splits}-Fold Time Series Cross-Validation with Tweedie...\")\n",
        "\n",
        "rmse_scores = []\n",
        "fold = 1\n",
        "\n",
        "for train_index, val_index in tscv.split(X):\n",
        "    print(f\"--- Fold {fold}/{n_splits} ---\")\n",
        "\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    print(f\"Training data size: {len(X_train)}\")\n",
        "    print(f\"Validation data size: {len(X_val)}\")\n",
        "\n",
        "    model = lgb.LGBMRegressor(\n",
        "        # core params\n",
        "        objective='tweedie',  # use 'tweedie': perfect for sales data (counts with many zeros)\n",
        "        metric='rmse',        # tell the model to optimize for RMSE (Root Mean Squared Error)\n",
        "        # device='gpu',\n",
        "        # max_bin=255,\n",
        "\n",
        "        # speed and performance\n",
        "        n_estimators=1000,    # build up to 1000 simple \"decision trees\" (will stop early)\n",
        "        learning_rate=0.03,   # how quickly the model learns\n",
        "        n_jobs=-1,            # use all available CPU cores to train faster\n",
        "\n",
        "        # overfitting guardrails\n",
        "        subsample=0.8,        # use 80% of rows for each tree\n",
        "        colsample_bytree=0.8, # use 80% of features for each tree\n",
        "\n",
        "        random_state=42       # Ensures getting the exact same results every time run this\n",
        "    )\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_val, y_val)],\n",
        "              eval_metric='rmse',\n",
        "              callbacks=[lgb.early_stopping(50, verbose=False)],\n",
        "              # specify LightGBM which features are categorical (very very important part)\n",
        "              categorical_feature=categorical_features\n",
        "             )\n",
        "\n",
        "    val_preds = model.predict(X_val)\n",
        "\n",
        "    val_preds[val_preds < 0] = 0 # sales cannot be negative\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "    print(f\"Fold {fold} RMSE: {rmse:.4f}\\n\")\n",
        "    rmse_scores.append(rmse)\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "print(\"--- Cross-Validation Complete ---\")\n",
        "print(f\"Mean RMSE across {n_splits} folds: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Std Dev of RMSE across {n_splits} folds: {np.std(rmse_scores):.4f}\")\n",
        "print(\"\\nIndividual Fold RMSEs:\")\n",
        "print(rmse_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeA1zN2__8fZ",
        "outputId": "0e34eb4c-d384-4730-c654-c9982f895d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape before dropna: (58327370, 32)\n",
            "New shape after dropna: (58327370, 32)\n",
            "\n",
            "Starting 5-Fold Time Series Cross-Validation with Tweedie...\n",
            "--- Fold 1/5 ---\n",
            "Training data size: 9721230\n",
            "Validation data size: 9721228\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.536613 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2106\n",
            "[LightGBM] [Info] Number of data points in the train set: 9721230, number of used features: 27\n",
            "[LightGBM] [Info] Start training from score 0.083667\n",
            "Fold 1 RMSE: 4.3457\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "Training data size: 19442458\n",
            "Validation data size: 9721228\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.068622 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2685\n",
            "[LightGBM] [Info] Number of data points in the train set: 19442458, number of used features: 27\n",
            "[LightGBM] [Info] Start training from score 0.365451\n",
            "Fold 2 RMSE: 3.2105\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "Training data size: 29163686\n",
            "Validation data size: 9721228\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.377678 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3201\n",
            "[LightGBM] [Info] Number of data points in the train set: 29163686, number of used features: 27\n",
            "[LightGBM] [Info] Start training from score 0.470899\n",
            "Fold 3 RMSE: 1.5723\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "Training data size: 38884914\n",
            "Validation data size: 9721228\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.817909 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3683\n",
            "[LightGBM] [Info] Number of data points in the train set: 38884914, number of used features: 27\n",
            "[LightGBM] [Info] Start training from score 0.293322\n",
            "Fold 4 RMSE: 1.7747\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "Training data size: 48606142\n",
            "Validation data size: 9721228\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.240192 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4169\n",
            "[LightGBM] [Info] Number of data points in the train set: 48606142, number of used features: 27\n",
            "[LightGBM] [Info] Start training from score 0.262921\n",
            "Fold 5 RMSE: 0.7092\n",
            "\n",
            "--- Cross-Validation Complete ---\n",
            "Mean RMSE across 5 folds: 2.3225\n",
            "Std Dev of RMSE across 5 folds: 1.2921\n",
            "\n",
            "Individual Fold RMSEs:\n",
            "[np.float64(4.345670451157473), np.float64(3.2105224047156176), np.float64(1.5722826759590647), np.float64(1.7747290321824476), np.float64(0.7092036624818026)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
        "                        'weekday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
        "\n",
        "numeric_features = ['wday', 'month', 'year', 'day_of_week', 'day_of_month', 'day_of_year',\n",
        "                    'weekend', 'snap_CA', 'snap_TX', 'snap_WI',\n",
        "                    'sell_price', 'price_mean', 'price_relative_to_mean', 'price_change_lag', 'sales_lag_7', 'sales_lag_28', 'sales_rolling_mean_28']\n",
        "\n",
        "features = categorical_features + numeric_features\n",
        "\n",
        "target = 'sales'\n",
        "\n",
        "print(f\"Original shape before dropna: {df.shape}\")\n",
        "df.dropna(inplace=True)\n",
        "print(f\"New shape after dropna: {df.shape}\")\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "n_splits = 5\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "print(f\"\\nStarting {n_splits}-Fold Time Series Cross-Validation with Tweedie...\")\n",
        "\n",
        "rmse_scores = []\n",
        "fold = 1\n",
        "\n",
        "for train_index, val_index in tscv.split(X):\n",
        "    print(f\"--- Fold {fold}/{n_splits} ---\")\n",
        "\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    print(f\"Training data size: {len(X_train)}\")\n",
        "    print(f\"Validation data size: {len(X_val)}\")\n",
        "\n",
        "    model = lgb.LGBMRegressor(\n",
        "        # core params\n",
        "        objective='tweedie',  # use 'tweedie': perfect for sales data (counts with many zeros)\n",
        "        metric='rmse',        # tell the model to optimize for RMSE (Root Mean Squared Error)\n",
        "        # device='gpu',\n",
        "        # max_bin=255,\n",
        "\n",
        "        # speed and performance\n",
        "        n_estimators=1000,    # build up to 1000 simple \"decision trees\" (will stop early)\n",
        "        learning_rate=0.05,   # how quickly the model learns -> 0.05 is a good choice.\n",
        "        n_jobs=-1,            # use all available CPU cores to train faster\n",
        "\n",
        "        # overfitting guardrails\n",
        "        subsample=0.8,        # use 80% of rows for each tree\n",
        "        colsample_bytree=0.8, # use 80% of features for each tree\n",
        "\n",
        "        random_state=42       # Ensures getting the exact same results every time run this\n",
        "    )\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_val, y_val)],\n",
        "              eval_metric='rmse',\n",
        "              callbacks=[lgb.early_stopping(50, verbose=False)],\n",
        "              # specify LightGBM which features are categorical (very very important part)\n",
        "              categorical_feature=categorical_features\n",
        "             )\n",
        "\n",
        "    val_preds = model.predict(X_val)\n",
        "\n",
        "    val_preds[val_preds < 0] = 0 # sales cannot be negative\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "    print(f\"Fold {fold} RMSE: {rmse:.4f}\\n\")\n",
        "    rmse_scores.append(rmse)\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "print(\"--- Cross-Validation Complete ---\")\n",
        "print(f\"Mean RMSE across {n_splits} folds: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Std Dev of RMSE across {n_splits} folds: {np.std(rmse_scores):.4f}\")\n",
        "print(\"\\nIndividual Fold RMSEs:\")\n",
        "print(rmse_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo6RYjnN4r1_",
        "outputId": "c52d7315-5e90-43ee-dfc2-95affbb6ce2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape before dropna: (59181090, 32)\n",
            "New shape after dropna: (58327370, 32)\n",
            "\n",
            "Starting 5-Fold Time Series Cross-Validation with Tweedie...\n",
            "--- Fold 1/5 ---\n",
            "Training data size: 9721230\n",
            "Validation data size: 9721228\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.550373 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2106\n",
            "[LightGBM] [Info] Number of data points in the train set: 9721230, number of used features: 27\n",
            "[LightGBM] [Info] Start training from score 0.083667\n",
            "Fold 1 RMSE: 4.3262\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "Training data size: 19442458\n",
            "Validation data size: 9721228\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.841234 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2685\n",
            "[LightGBM] [Info] Number of data points in the train set: 19442458, number of used features: 27\n",
            "[LightGBM] [Info] Start training from score 0.365451\n",
            "Fold 2 RMSE: 3.1800\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "Training data size: 29163686\n",
            "Validation data size: 9721228\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.390643 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3201\n",
            "[LightGBM] [Info] Number of data points in the train set: 29163686, number of used features: 27\n",
            "[LightGBM] [Info] Start training from score 0.470899\n",
            "Fold 3 RMSE: 1.5727\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "Training data size: 38884914\n",
            "Validation data size: 9721228\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.937419 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3683\n",
            "[LightGBM] [Info] Number of data points in the train set: 38884914, number of used features: 27\n",
            "[LightGBM] [Info] Start training from score 0.293322\n",
            "Fold 4 RMSE: 1.7752\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "Training data size: 48606142\n",
            "Validation data size: 9721228\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.306204 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4169\n",
            "[LightGBM] [Info] Number of data points in the train set: 48606142, number of used features: 27\n",
            "[LightGBM] [Info] Start training from score 0.262921\n",
            "Fold 5 RMSE: 0.7092\n",
            "\n",
            "--- Cross-Validation Complete ---\n",
            "Mean RMSE across 5 folds: 2.3127\n",
            "Std Dev of RMSE across 5 folds: 1.2817\n",
            "\n",
            "Individual Fold RMSEs:\n",
            "[np.float64(4.326151182299157), np.float64(3.180049813523074), np.float64(1.572720076578302), np.float64(1.775238652491532), np.float64(0.7091792495220123)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
        "                        'weekday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
        "\n",
        "numeric_features = ['wday', 'month', 'year', 'day_of_week', 'day_of_month', 'day_of_year',\n",
        "                    'weekend', 'snap_CA', 'snap_TX', 'snap_WI',\n",
        "                    'sell_price', 'price_mean', 'price_relative_to_mean', 'price_change_lag']\n",
        "\n",
        "features = categorical_features + numeric_features\n",
        "\n",
        "target = 'sales'\n",
        "\n",
        "print(f\"Original shape before dropna: {df.shape}\")\n",
        "df.dropna(inplace=True)\n",
        "print(f\"New shape after dropna: {df.shape}\")\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "n_splits = 5\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "print(f\"\\nStarting {n_splits}-Fold Time Series Cross-Validation with Tweedie...\")\n",
        "\n",
        "rmse_scores = []\n",
        "fold = 1\n",
        "\n",
        "for train_index, val_index in tscv.split(X):\n",
        "    print(f\"--- Fold {fold}/{n_splits} ---\")\n",
        "\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    print(f\"Training data size: {len(X_train)}\")\n",
        "    print(f\"Validation data size: {len(X_val)}\")\n",
        "\n",
        "    model = lgb.LGBMRegressor(\n",
        "        # core params\n",
        "        objective='tweedie',  # use 'tweedie': perfect for sales data (counts with many zeros)\n",
        "        metric='rmse',        # tell the model to optimize for RMSE (Root Mean Squared Error)\n",
        "        # device='gpu',\n",
        "        # max_bin=255,\n",
        "\n",
        "        # speed and performance\n",
        "        n_estimators=1000,    # build up to 1000 simple \"decision trees\" (will stop early)\n",
        "        learning_rate=0.05,   # how quickly the model learns -> 0.05 is a good choice.\n",
        "        n_jobs=-1,            # use all available CPU cores to train faster\n",
        "\n",
        "        # overfitting guardrails\n",
        "        subsample=0.8,        # use 80% of rows for each tree\n",
        "        colsample_bytree=0.8, # use 80% of features for each tree\n",
        "\n",
        "        random_state=42       # Ensures getting the exact same results every time run this\n",
        "    )\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_val, y_val)],\n",
        "              eval_metric='rmse',\n",
        "              callbacks=[lgb.early_stopping(50, verbose=False)],\n",
        "              # specify LightGBM which features are categorical (very very important part)\n",
        "              categorical_feature=categorical_features\n",
        "             )\n",
        "\n",
        "    val_preds = model.predict(X_val)\n",
        "\n",
        "    val_preds[val_preds < 0] = 0 # sales cannot be negative\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "    print(f\"Fold {fold} RMSE: {rmse:.4f}\\n\")\n",
        "    rmse_scores.append(rmse)\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "print(\"--- Cross-Validation Complete ---\")\n",
        "print(f\"Mean RMSE across {n_splits} folds: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Std Dev of RMSE across {n_splits} folds: {np.std(rmse_scores):.4f}\")\n",
        "print(\"\\nIndividual Fold RMSEs:\")\n",
        "print(rmse_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qkszlg2tujRa",
        "outputId": "58471076-130f-43f9-8828-2c0418284f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape before dropna: (59181090, 29)\n",
            "New shape after dropna: (59181090, 29)\n",
            "\n",
            "Starting 5-Fold Time Series Cross-Validation with Tweedie...\n",
            "--- Fold 1/5 ---\n",
            "Training data size: 9863515\n",
            "Validation data size: 9863515\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.355888 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1744\n",
            "[LightGBM] [Info] Number of data points in the train set: 9863515, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score 0.080907\n",
            "Fold 1 RMSE: 6.0155\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "Training data size: 19727030\n",
            "Validation data size: 9863515\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.668574 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2236\n",
            "[LightGBM] [Info] Number of data points in the train set: 19727030, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score 0.362749\n",
            "Fold 2 RMSE: 5.6752\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "Training data size: 29590545\n",
            "Validation data size: 9863515\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.037235 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2737\n",
            "[LightGBM] [Info] Number of data points in the train set: 29590545, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score 0.467928\n",
            "Fold 3 RMSE: 1.9722\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "Training data size: 39454060\n",
            "Validation data size: 9863515\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.901304 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3234\n",
            "[LightGBM] [Info] Number of data points in the train set: 39454060, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score 0.290435\n",
            "Fold 4 RMSE: 2.6805\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "Training data size: 49317575\n",
            "Validation data size: 9863515\n",
            "Training model...\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.003314 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3741\n",
            "[LightGBM] [Info] Number of data points in the train set: 49317575, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score 0.259362\n",
            "Fold 5 RMSE: 0.8889\n",
            "\n",
            "--- Cross-Validation Complete ---\n",
            "Mean RMSE across 5 folds: 3.4464\n",
            "Std Dev of RMSE across 5 folds: 2.0430\n",
            "\n",
            "Individual Fold RMSEs:\n",
            "[np.float64(6.015495228490148), np.float64(5.675159014221771), np.float64(1.9721585384492206), np.float64(2.6804973653766853), np.float64(0.8888765510898942)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 24 features (before adding lag and rolling features)\n",
        "\n",
        "Original shape before dropna: (59181090, 29)\n",
        "New shape after dropna: (59181090, 29)\n",
        "\n",
        "Starting 5-Fold Time Series Cross-Validation with Tweedie...\n",
        "--- Fold 1/5 ---\n",
        "Training data size: 9863515\n",
        "Validation data size: 9863515\n",
        "Training model...\n",
        "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
        "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
        "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.355888 seconds.\n",
        "You can set `force_row_wise=true` to remove the overhead.\n",
        "And if memory is not enough, you can set `force_col_wise=true`.\n",
        "[LightGBM] [Info] Total Bins 1744\n",
        "[LightGBM] [Info] Number of data points in the train set: 9863515, number of used features: 24\n",
        "[LightGBM] [Info] Start training from score 0.080907\n",
        "Fold 1 RMSE: 6.0155\n",
        "\n",
        "--- Fold 2/5 ---\n",
        "Training data size: 19727030\n",
        "Validation data size: 9863515\n",
        "Training model...\n",
        "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
        "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
        "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.668574 seconds.\n",
        "You can set `force_row_wise=true` to remove the overhead.\n",
        "And if memory is not enough, you can set `force_col_wise=true`.\n",
        "[LightGBM] [Info] Total Bins 2236\n",
        "[LightGBM] [Info] Number of data points in the train set: 19727030, number of used features: 24\n",
        "[LightGBM] [Info] Start training from score 0.362749\n",
        "Fold 2 RMSE: 5.6752\n",
        "\n",
        "--- Fold 3/5 ---\n",
        "Training data size: 29590545\n",
        "Validation data size: 9863515\n",
        "Training model...\n",
        "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
        "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
        "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.037235 seconds.\n",
        "You can set `force_row_wise=true` to remove the overhead.\n",
        "And if memory is not enough, you can set `force_col_wise=true`.\n",
        "[LightGBM] [Info] Total Bins 2737\n",
        "[LightGBM] [Info] Number of data points in the train set: 29590545, number of used features: 24\n",
        "[LightGBM] [Info] Start training from score 0.467928\n",
        "Fold 3 RMSE: 1.9722\n",
        "\n",
        "--- Fold 4/5 ---\n",
        "Training data size: 39454060\n",
        "Validation data size: 9863515\n",
        "Training model...\n",
        "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
        "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
        "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.901304 seconds.\n",
        "You can set `force_row_wise=true` to remove the overhead.\n",
        "And if memory is not enough, you can set `force_col_wise=true`.\n",
        "[LightGBM] [Info] Total Bins 3234\n",
        "[LightGBM] [Info] Number of data points in the train set: 39454060, number of used features: 24\n",
        "[LightGBM] [Info] Start training from score 0.290435\n",
        "Fold 4 RMSE: 2.6805\n",
        "\n",
        "--- Fold 5/5 ---\n",
        "Training data size: 49317575\n",
        "Validation data size: 9863515\n",
        "Training model...\n",
        "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
        "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
        "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.003314 seconds.\n",
        "You can set `force_row_wise=true` to remove the overhead.\n",
        "And if memory is not enough, you can set `force_col_wise=true`.\n",
        "[LightGBM] [Info] Total Bins 3741\n",
        "[LightGBM] [Info] Number of data points in the train set: 49317575, number of used features: 24\n",
        "[LightGBM] [Info] Start training from score 0.259362\n",
        "Fold 5 RMSE: 0.8889\n",
        "\n",
        "--- Cross-Validation Complete ---\n",
        "Mean RMSE across 5 folds: 3.4464\n",
        "Std Dev of RMSE across 5 folds: 2.0430\n",
        "\n",
        "Individual Fold RMSEs:\n",
        "[np.float64(6.015495228490148), np.float64(5.675159014221771), np.float64(1.9721585384492206), np.float64(2.6804973653766853), np.float64(0.8888765510898942)]\n"
      ],
      "metadata": {
        "id": "PMUj-d3R36CG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean RMSE across 5 folds: 3.4464: This is the main performance score. On average, across all 5 folds, the model's prediction of sales for any given item on any given day was off by 3.45 units.\n",
        "\n",
        "Std Dev of RMSE... 2.0430: This number is quite high, and it's explained by the individual fold scores. It means the model's performance wasn't consistent across the different time periods.\n",
        "\n",
        "Individual Fold RMSEs: [6.01, 5.67, 1.97, 2.68, 0.88]: This is the most important part.\n",
        "\n",
        "Model performed poorly on the early data (Folds 1 and 2, RMSE of ~6.0). It had less data to learn from and was trying to predict farther into the future.\n",
        "\n",
        "It got significantly better as it got more data.\n",
        "\n",
        "The last fold (Fold 5) is the most important one. It used the most data (49 million rows) to predict the most recent time period. An RMSE of 0.89 is an excellent score. It means that by the end, the model was, on average, off by less than one unit.\n",
        "\n",
        "-> model is learning very effectively. The high overall average (3.44) is just skewed by the early, \"dumber\" folds. The \"true\" model performance, when trained on all this data, is likely closer to the 0.89 RMSE."
      ],
      "metadata": {
        "id": "VIh9sh7b22dG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUq6vjgV4teg"
      },
      "outputs": [],
      "source": [
        "# features = [\n",
        "#     'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
        "#     'wday', 'month', 'year', 'day_of_week', 'day_of_month', 'day_of_year',\n",
        "#     'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
        "#     'snap_CA', 'snap_TX', 'snap_WI',\n",
        "#     'sell_price', 'price_relative_to_mean', 'price_change_lag'\n",
        "# ]\n",
        "\n",
        "# target = 'sales'\n",
        "\n",
        "# df_train = df.dropna(subset=['sales'])\n",
        "# X = df_train[features]\n",
        "# y = df_train[target]\n",
        "\n",
        "# n_splits = 10\n",
        "# tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# print(f\"Starting {n_splits}-Fold Time Series Cross-Validation...\")\n",
        "\n",
        "# rmse_scores = []\n",
        "# fold = 1\n",
        "\n",
        "# for train_index, val_index in tscv.split(X):\n",
        "#     print(f\"--- Fold {fold}/{n_splits} ---\")\n",
        "\n",
        "#     X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "#     y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "#     print(f\"Training data size: {len(X_train)}\")\n",
        "#     print(f\"Validation data size: {len(X_val)}\")\n",
        "\n",
        "#     # LightGBM model with Tweedie regression.\n",
        "#     # because it is excellent for 'count' data with many zeros, like sales.\n",
        "#     model = lgb.LGBMRegressor(\n",
        "#         objective='tweedie',\n",
        "#         metric='rmse',\n",
        "#         n_estimators=1000,\n",
        "#         learning_rate=0.05,\n",
        "#         num_leaves=31,\n",
        "#         subsample=0.8,\n",
        "#         colsample_bytree=0.8,\n",
        "#         random_state=42,\n",
        "#         n_jobs=-1\n",
        "#     )\n",
        "\n",
        "#     print(\"Training model...\")\n",
        "#     model.fit(X_train, y_train,\n",
        "#               eval_set=[(X_val, y_val)],\n",
        "#               eval_metric='rmse',\n",
        "#               callbacks=[lgb.early_stopping(50, verbose=False)])\n",
        "\n",
        "#     val_preds = model.predict(X_val)\n",
        "\n",
        "#     val_preds[val_preds < 0] = 0\n",
        "\n",
        "#     rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "#     print(f\"Fold {fold} RMSE: {rmse}\\n\")\n",
        "#     rmse_scores.append(rmse)\n",
        "\n",
        "#     fold += 1\n",
        "\n",
        "# print(\"\\n--- Cross-Validation Complete ---\")\n",
        "# print(f\"Mean RMSE across {n_splits} folds: {np.mean(rmse_scores):.4f}\")\n",
        "# print(f\"Std Dev of RMSE across {n_splits} folds: {np.std(rmse_scores):.4f}\")\n",
        "# print(\"\\nIndividual Fold RMSEs:\")\n",
        "# print(rmse_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPStzKgM5cpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba0be87-f576-4690-b5d8-4cb4508046ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting 5-Fold Time Series Cross-Validation with Ridge Regression...\n",
            "--- Fold 1/5 ---\n",
            "Training data size: 9863515\n",
            "Validation data size: 9863515\n",
            "Training model...\n",
            "Evaluating model...\n",
            "Fold 1 RMSE: 5.968871816043665\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "Training data size: 19727030\n",
            "Validation data size: 9863515\n",
            "Training model...\n"
          ]
        }
      ],
      "source": [
        "# --- Load Data (Assuming this is already done) ---\n",
        "# calendar_df = pd.read_csv('calendar.csv')\n",
        "# prices_df = pd.read_csv('sell_prices.csv')\n",
        "# sales_df = pd.read_csv('sales_train_validation.csv')\n",
        "# ... (all your data merging and feature engineering from above) ...\n",
        "# df = ... (your final merged and cleaned DataFrame)\n",
        "\n",
        "# Let's assume 'df' is the DataFrame created in your previous steps.\n",
        "# We'll drop rows with NaNs from the lag/rolling features you created.\n",
        "# This also removes the first 27 days for which sales_lag_28 is undefined.\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Define feature columns (X) and target column (y)\n",
        "# We will exclude 'id', 'd', 'date', and 'wm_yr_wk' (since 'year' and 'month' capture time)\n",
        "target = 'sales'\n",
        "categorical_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
        "                        'weekday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
        "numeric_features = ['wday', 'month', 'year', 'day_of_week', 'day_of_month', 'day_of_year',\n",
        "                    'weekend', 'snap_CA', 'snap_TX', 'snap_WI',\n",
        "                    'sell_price', 'price_mean', 'price_relative_to_mean', 'price_change_lag']\n",
        "\n",
        "# Combine all features\n",
        "features = categorical_features + numeric_features\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Create a preprocessor\n",
        "# OneHotEncoder for categorical features\n",
        "# 'passthrough' for numeric features (they are already scaled or are simple counts/flags)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
        "        ('num', 'passthrough', numeric_features)\n",
        "    ],\n",
        "    remainder='drop'  # Drop any columns not specified\n",
        ")\n",
        "\n",
        "# Create the model pipeline\n",
        "# 1. Preprocess the data (OneHotEncoding)\n",
        "# 2. Apply Ridge Regression (a good linear baseline)\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                        ('regressor', Ridge(alpha=1.0, random_state=42))])\n",
        "\n",
        "# --- Time Series Cross-Validation ---\n",
        "# The user's request for \"80% training and 20% testing with cross-validation\"\n",
        "# is best implemented using TimeSeriesSplit.\n",
        "# With n_splits=5, the first fold is ~17% train, 83% test.\n",
        "# The last fold is ~83% train, 17% test, which is close to the 80/20 split requested.\n",
        "\n",
        "n_splits = 5  # Using 5 splits instead of 10 to manage computational time\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "print(f\"Starting {n_splits}-Fold Time Series Cross-Validation with Ridge Regression...\")\n",
        "\n",
        "rmse_scores = []\n",
        "fold = 1\n",
        "\n",
        "for train_index, val_index in tscv.split(X):\n",
        "    print(f\"--- Fold {fold}/{n_splits} ---\")\n",
        "\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    print(f\"Training data size: {len(X_train)}\")\n",
        "    print(f\"Validation data size: {len(X_val)}\")\n",
        "\n",
        "    # Fit the pipeline\n",
        "    print(\"Training model...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on validation data\n",
        "    print(\"Evaluating model...\")\n",
        "    val_preds = model.predict(X_val)\n",
        "\n",
        "    # Ensure predictions are non-negative\n",
        "    val_preds[val_preds < 0] = 0\n",
        "\n",
        "    # Calculate RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "    print(f\"Fold {fold} RMSE: {rmse}\\n\")\n",
        "    rmse_scores.append(rmse)\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "print(\"\\n--- Cross-Validation Complete ---\")\n",
        "print(f\"Mean RMSE across {n_splits} folds: {np.mean(rmse_scores):.4f}\")\n",
        "print(f\"Std Dev of RMSE across {n_splits} folds: {np.std(rmse_scores):.4f}\")\n",
        "print(\"\\nIndividual Fold RMSEs:\")\n",
        "print(rmse_scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NSrLUcffFf90"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOCjb9ld/kR02bWvi0voVpe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}